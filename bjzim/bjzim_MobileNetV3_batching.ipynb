{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Importiere benötigte Bibliotheken ----\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tensorflow.keras import layers, Model, regularizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV3Large\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaSgmHzMsoh"
      },
      "outputs": [],
      "source": [
        "# ---- Setze Konstanten und Parameter ----\n",
        "SEED = 42\n",
        "NUM_EPOCHS = 150\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = (224, 224)\n",
        "TARGET_LABEL = \"dx\"\n",
        "BALANCE_LABEL = \"dx\"\n",
        "FILEPATH_JPGS = './../data/jpgs/'\n",
        "FILEPATH_PROCESSED = './../data/processed/'\n",
        "FILEPATH_OUTPUT = './../data/bjzim-models/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Initialisiere sonstige Variablen ----\n",
        "pbar = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClM_Z44mMumW"
      },
      "outputs": [],
      "source": [
        "# ---- Dateipfade und Set-Namen ----\n",
        "filepaths = [\n",
        "    (\"Trainingsset\", FILEPATH_PROCESSED + \"train_from_Metadata_processed.csv\"),\n",
        "    (\"Validierungsset\", FILEPATH_PROCESSED + \"validation_from_Metadata_processed.csv\"),\n",
        "    (\"Testset\", FILEPATH_PROCESSED + \"test_from_Metadata_processed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Funktion zur Überprüfung von augmentierten Daten ----\n",
        "def check_augmented_data(df, set_name):\n",
        "    if df['image_id'].str.startswith('aug_').any():\n",
        "        print(f\"Warnung: Augmentierte Daten im {set_name} gefunden.\")\n",
        "\n",
        "# ---- Überprüfung ----\n",
        "for set_name, filepath in filepaths:\n",
        "    df = pd.read_csv(filepath)\n",
        "    check_augmented_data(df, set_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your augmentation parameters\n",
        "\n",
        "\n",
        "# aug_params_loss = {\n",
        "#     'height_shift_range': 0.1,\n",
        "#     'horizontal_flip': True,\n",
        "#     'rotation_range': 0,\n",
        "#     'vertical_flip': True,\n",
        "#     'width_shift_range': 0.2,\n",
        "#     'zoom_range': 0.05\n",
        "# }\n",
        "\n",
        "aug_params_recall = {\n",
        "    'height_shift_range': 0.05,\n",
        "    'horizontal_flip': False,\n",
        "    'rotation_range': 30,\n",
        "    'vertical_flip': True,\n",
        "    'width_shift_range': 0.2,\n",
        "    'zoom_range': 0.05\n",
        "}\n",
        "# Create a grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'conv2d_filters': [128],\n",
        "    'dense_units': [64, 128],\n",
        "    'dropout_rate': [0.5, 0.7],\n",
        "    'batch_size': [64],\n",
        "    'optimizer': ['Adam'],\n",
        "    'weight_regularization': ['l2']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen_train = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=aug_params_recall['rotation_range'],\n",
        "    width_shift_range=aug_params_recall['width_shift_range'],\n",
        "    height_shift_range=aug_params_recall['height_shift_range'],\n",
        "    zoom_range=aug_params_recall['zoom_range'],\n",
        "    horizontal_flip=aug_params_recall['horizontal_flip'],\n",
        "    vertical_flip=aug_params_recall['vertical_flip'],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_data_generator = datagen_train.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=FILEPATH_JPGS,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=TARGET_LABEL,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "datagen_validation = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0, #see above\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "validation_generator = datagen_validation.flow_from_dataframe(\n",
        "    dataframe=validation_df,\n",
        "    directory=FILEPATH_JPGS,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=TARGET_LABEL,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_f1',\n",
        "    mode='max',\n",
        "    patience=20, #15,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_f1',\n",
        "    mode='max',\n",
        "    factor=0.5, #0.1,\n",
        "    patience=12, #8,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'model_best_weights.h5', \n",
        "    save_best_only=True, \n",
        "    save_weights_only=True, \n",
        "    monitor='val_f1', \n",
        "    mode='max', \n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mit GlobalAveragepooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomMetrics(Callback):\n",
        "    def __init__(self, validation_generator):\n",
        "        super().__init__()\n",
        "        self.validation_generator = validation_generator\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        val_preds = np.argmax(self.model.predict(self.validation_generator), axis=1)\n",
        "        val_true = self.validation_generator.classes\n",
        "        val_recall = recall_score(val_true, val_preds, average='weighted')\n",
        "        val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
        "        val_auc = roc_auc_score(val_true, self.model.predict(self.validation_generator), multi_class='ovr', average='weighted')\n",
        "        logs['val_recall'] = val_recall\n",
        "        logs['val_f1'] = val_f1\n",
        "        logs['val_auc'] = val_auc\n",
        "        print(f\" - val_recall: {val_recall: .5f} - val_f1: {val_f1: .5f} - val_auc: {val_auc: .5f}\")\n",
        "        print(\"-----------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_evaluate_model(params, train_df, validation_df, FILEPATH_JPGS, TARGET_LABEL, IMAGE_SIZE, BATCH_SIZE):\n",
        "\n",
        "    base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "    # Unfreeze some of the 269 layers for fine-tuning\n",
        "    for layer in base_model.layers[:150]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[150:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Regularization\n",
        "    reg_type = params.get('weight_regularization', None)\n",
        "    if reg_type == 'l1':\n",
        "        reg = regularizers.l1(0.01)\n",
        "    elif reg_type == 'l2':\n",
        "        reg = regularizers.l2(0.01)\n",
        "    else:\n",
        "        reg = None\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = layers.Dense(params['dense_units'], activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.Dropout(params['dropout_rate'])(x)\n",
        "    x = layers.Dense(params['dense_units'] // 2, activation='relu', kernel_regularizer=reg)(x)  # Zusätzlicher Dense-Layer\n",
        "    x = layers.Dropout(params['dropout_rate'])(x)  # Zusätzlicher Dropout-Layer\n",
        "    x = layers.Dense(7, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=params['learning_rate']), \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Define data generators with augmentation\n",
        "    datagen_train = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rotation_range=aug_params_recall['rotation_range'],\n",
        "        width_shift_range=aug_params_recall['width_shift_range'],\n",
        "        height_shift_range=aug_params_recall['height_shift_range'],\n",
        "        zoom_range=aug_params_recall['zoom_range'],\n",
        "        horizontal_flip=aug_params_recall['horizontal_flip'],\n",
        "        vertical_flip=aug_params_recall['vertical_flip'],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    datagen_validation = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        preprocessing_function=preprocess_input\n",
        "    )\n",
        "\n",
        "    train_generator = datagen_train.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=FILEPATH_JPGS,\n",
        "        x_col=\"image_id\",\n",
        "        y_col=TARGET_LABEL,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=True,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen_validation.flow_from_dataframe(\n",
        "        dataframe=validation_df,\n",
        "        directory=FILEPATH_JPGS,\n",
        "        x_col=\"image_id\",\n",
        "        y_col=TARGET_LABEL,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=False,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    custom_metrics = CustomMetrics(validation_generator=validation_generator)\n",
        "\n",
        "    # Train the model with callbacks\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=validation_generator,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks=[custom_metrics, early_stopping, reduce_lr, model_checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "\n",
        "    # Calculate F1 score\n",
        "    val_preds = model.predict(validation_generator)\n",
        "    val_true_labels = validation_generator.labels  # Änderung hier\n",
        "    val_pred_labels = np.argmax(val_preds, axis=1)  # Neue Zeile\n",
        "    f1 = f1_score(val_true_labels, val_pred_labels, average='weighted')\n",
        "\n",
        "    return model, f1, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize variables\n",
        "results_df = pd.DataFrame(columns=['learning_rate', 'conv2d_filters', 'dense_units', 'dropout_rate', 'val_f1'])\n",
        "best_models, completed_iterations = [], 0\n",
        "total_iterations = len(ParameterGrid(param_grid))\n",
        "\n",
        "def print_remaining_iterations(total, completed):\n",
        "    return f\"{total - completed} iterations remaining\"\n",
        "\n",
        "# Main loop for hyperparameter tuning\n",
        "pbar = tqdm(total=total_iterations, desc=\"Hyperparameter Optimization\")\n",
        "for params in tqdm(ParameterGrid(param_grid), total=total_iterations, desc=\"Hyperparameter Optimization\"):\n",
        "    model, f1, _ = train_evaluate_model(params, train_df, validation_df, FILEPATH_JPGS, TARGET_LABEL, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "    # Update results DataFrame and best models list\n",
        "    results_df = results_df.append({**params, 'val_f1': f1}, ignore_index=True)\n",
        "    best_models = sorted(best_models + [(f1, model)], key=lambda x: x[0], reverse=True)[:5]\n",
        "    \n",
        "    # Progress update\n",
        "    completed_iterations += 1\n",
        "    print(print_remaining_iterations(total_iterations, completed_iterations))\n",
        "    pbar.update(1)\n",
        "pbar.close()\n",
        "\n",
        "# Save the top 5 models and results DataFrame\n",
        "for i, (f1, model) in enumerate(best_models):\n",
        "    model.save(f'best_model_{i + 1}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.h5')\n",
        "    print(f\"Saved best_model_{i + 1} with F1: {f1}\")\n",
        "    \n",
        "results_df.to_csv(FILEPATH_OUTPUT + 'hyperparameter_tuning_results.csv', index=False)\n",
        "\n",
        "print(\"Best F1 Scores:\", [f1 for f1, _ in best_models])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show me results sorted by val_f1\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(model.layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotte die Lernkurven\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotte die Genauigkeit\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plotte den Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mit BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_compile_model(learning_rate, conv2d_filters, dense_units, dropout_rate):\n",
        "    # Initialize the MobileNetV3Large model with fixed layers\n",
        "    base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "    # Unfreeze some of the layers for fine-tuning\n",
        "    for layer in base_model.layers[:100]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[100:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Add custom layers on top\n",
        "    x = layers.Conv2D(conv2d_filters, (3, 3), activation='relu')(base_model.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(dense_units, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(7, activation='softmax')(x)\n",
        "\n",
        "    # Create the full model\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizing Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd5qtmFQcIRB"
      },
      "source": [
        "## **Build Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIhFCOFSPrKg"
      },
      "outputs": [],
      "source": [
        "# Initialize the MobileNetV3Large model\n",
        "base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "# Unfreeze some of the layers for fine-tuning\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[100:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "# Add custom layers on top\n",
        "x = layers.Conv2D(128, (3, 3), activation='relu')(base_model.output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(7, activation='softmax')(x)\n",
        "\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=base_model.input, outputs=x, name=\"MobilneNetV3Large_pretrained-weights_fixed-layers_custom-conv2D\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Grid: Data Preprocessing and Augmentation, Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Beste Metrik initialisieren\n",
        "best_val_recall = 0.0\n",
        "best_params = None\n",
        "best_model_path = \"best_model_recall.h5\"\n",
        "\n",
        "# Bestehende Erkenntnisse laden\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "\n",
        "# Modell-Kompilierung (vor der Schleife, da die Lernrate konstant bleibt)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),  # Deine feste Lernrate\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        Precision(name='precision'),\n",
        "        Recall(name='recall'),\n",
        "        AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Erstelle eine Liste aller Kombinationen der Hyperparameter\n",
        "grid = ParameterGrid(param_grid)\n",
        "total_iterations = len(grid)  # Gesamtanzahl der Iterationen\n",
        "iteration = 1  # Aktuelle Iteration\n",
        "\n",
        "# Durchlaufe jede Kombination der Hyperparameter\n",
        "for params in grid:\n",
        "    print(f\"Training with params: {params}\")\n",
        "    print(f\"Iteration {iteration} of {total_iterations}\")\n",
        "    \n",
        "    # Definiere den TensorBoard-Log-Pfad für diese Hyperparameter-Kombination\n",
        "#    log_dir = f\"./tensorboard_logs/{params}\"\n",
        "#    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    # Erstelle einen neuen ImageDataGenerator mit den aktuellen Parametern\n",
        "    datagen_train = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rotation_range=params['rotation_range'],\n",
        "        width_shift_range=params['width_shift_range'],\n",
        "        height_shift_range=params['height_shift_range'],\n",
        "        zoom_range=params['zoom_range'],\n",
        "        horizontal_flip=params['horizontal_flip'],\n",
        "        vertical_flip=params['vertical_flip'],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    \n",
        "    train_data_generator = datagen_train.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=FILEPATH_JPGS,\n",
        "        x_col=\"image_id\",\n",
        "        y_col=TARGET_LABEL,\n",
        "        class_mode=\"categorical\",\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    \n",
        "    datagen_validation = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0, #see above\n",
        "        preprocessing_function=preprocess_input\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen_validation.flow_from_dataframe(\n",
        "        dataframe=validation_df,\n",
        "        directory=FILEPATH_JPGS,\n",
        "        x_col=\"image_id\",\n",
        "        y_col=TARGET_LABEL,\n",
        "        class_mode=\"categorical\",\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Trainiere das Modell mit den aktuellen Data Augmentation Parametern\n",
        "    # (Der Rest des Trainingscodes bleibt gleich)\n",
        "    # Train the model\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data_generator,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        verbose=1,                      # Adjust verbosity level\n",
        "        batch_size=batch_size,                # Set the batch size, default is 32, can be increased to speed up training, but memory consumption increases\n",
        "        callbacks=[early_stopping, reduce_lr],                 # removed tensorboard_callback\n",
        "        validation_split=0.0,           # not needed as we use a validation data generator\n",
        "        validation_data=validation_generator,\n",
        "        shuffle=True,                   # Shuffle the training data before each epoch\n",
        "        sample_weight=None,             # Set the weights for the train data set !\n",
        "        class_weight=None,              # Set the weights for the classes, not needed if we use sample weights\n",
        "        initial_epoch=0,                # Use this to continue training from a specific epoch\n",
        "        steps_per_epoch=None,           # Set the number of steps per epoch, default is len(x_train) // batch_size\n",
        "        validation_steps=None,          # Set the number of steps for validation, default is len(x_val) // batch_size\n",
        "        validation_batch_size=None,     # Set the batch size for validation, default is batch_size\n",
        "        validation_freq=1,              # Only relevant if validation data is a generator. Set the frequency to validate the model on the validation set\n",
        "        max_queue_size=10,              # Set the max size for the generator queue\n",
        "        workers=-1,                     # Set the max number of processes to generate the data in parallel, -1 means all CPUs\n",
        "        use_multiprocessing=True       # Set to True if you use a generator in parallel, e.g. model.predict_generator()\n",
        "    )\n",
        "\n",
        "    # Überprüfe, ob dieses Modell besser ist als das bisher beste\n",
        "    final_val_recall = history.history['val_recall'][-1]\n",
        "    if final_val_recall > best_val_recall:\n",
        "        print(f\"New best model found! val_recall: {final_val_recall}\")\n",
        "        best_val_recall = final_val_recall\n",
        "        best_params = params\n",
        "\n",
        "        # Speichere das beste Modell\n",
        "        model.save(best_model_path)\n",
        "\n",
        "    iteration += 1  # Aktualisiere die Iterationszählung\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lade das beste Modell\n",
        "best_model = load_model(best_model_path)\n",
        "\n",
        "\n",
        "print(f\"Best val_recall: {best_val_recall}\")\n",
        "print(f\"Best params: {best_params}\")\n",
        "print(f\"Best model path: {best_model_path}\")\n",
        "print(f\"Best model name: {best_model.name}\")\n",
        "print(f\"Best model optimizer: {best_model.optimizer}\")\n",
        "print(f\"Best model metrics: {best_model.metrics_names}\")\n",
        "print(f\"Best model loss & metrics: {best_model.evaluate(validation_generator)}\")\n",
        "print(f\"Best model summary: {best_model.summary()}\")\n",
        "print(f\"Best model layers: {best_model.layers}\")\n",
        "best_model_weights = best_model.get_weights()\n",
        "print(f\"Best model weights: {best_model_weights}\")\n",
        "print(f\"Best model history: {best_model.history}\")\n",
        "\n",
        "# sns heatmap for confusion matrix of best model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = best_model.predict(validation_generator)\n",
        "# Convert predictions classes to one hot vectors\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = validation_generator.classes\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        "\n",
        "# print classification report \n",
        "print('Classification Report')\n",
        "target_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "print(classification_report(validation_generator.classes, Y_pred_classes, target_names=target_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the confusion matrix\n",
        "f, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor=\"gray\", fmt='.1f', ax=ax)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "#subtitle what is swhon in confusion matrix\n",
        "plt.text(0.5, 0.5, \"\", horizontalalignment='center', verticalalignment='center', fontsize=18, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "\n",
        "#plot roc-auc curve for best model\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(len(target_names)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(np.array(validation_generator.classes)[:, i], Y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot of a ROC curve for a specific class\n",
        "for i in range(len(target_names)):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i], color='green')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', color='red')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
        "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
        "    plt.title('Receiver Operating Characteristic for ' + target_names[i])\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plot precision-recall curve for each class and iso-f1 curves\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# For each class\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "f1 = dict()\n",
        "average_precision = dict()\n",
        "for i in range(len(target_names)):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(validation_generator.classes[:, i], Y_pred[:, i])\n",
        "    average_precision[i] = average_precision_score(validation_generator.classes[:, i], Y_pred[:, i])\n",
        "    f1[i] = f1_score(validation_generator.classes[:, i], Y_pred_classes[:, i])\n",
        "\n",
        "# A \"micro-average\": quantifying score on all classes jointly\n",
        "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(validation_generator.classes.ravel(), Y_pred.ravel())\n",
        "average_precision[\"micro\"] = average_precision_score(validation_generator.classes, Y_pred, average=\"micro\")\n",
        "f1[\"micro\"] = f1_score(validation_generator.classes, Y_pred_classes, average=\"micro\")\n",
        "print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "print('Average F1 score, micro-averaged over all classes: {0:0.2f}'.format(f1[\"micro\"]))\n",
        "plt.figure()\n",
        "plt.step(recall['micro'], precision['micro'], where='post', color='green')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Average precision score, micro-averaged over all classes: AP={0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot all grid params against val_recall, val_precision, val_accuracy, val_auc, val_loss,f1\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Create a dataframe from the grid search results\n",
        "df_grid_aug = pd.DataFrame(grid)\n",
        "\n",
        "# Add the validation metrics to the dataframe\n",
        "df_grid_aug['val_recall'] = history.history['val_recall']\n",
        "df_grid_aug['val_precision'] = history.history['val_precision']\n",
        "df_grid_aug['val_accuracy'] = history.history['val_accuracy']\n",
        "df_grid_aug['val_auc'] = history.history['val_auc']\n",
        "df_grid_aug['val_loss'] = history.history['val_loss']\n",
        "\n",
        "# calculate the f1 score for the validation data as a metric and add it to the dataframe\n",
        "df_grid_aug['val_f1'] = 2 * (df_grid_aug['val_precision'] * df_grid_aug['val_recall']) / (df_grid_aug['val_precision'] + df_grid_aug['val_recall'])\n",
        "\n",
        "#show the dataframe in better optical order and sortable with jupyter ipython format, bad values in red good in green\n",
        "from IPython.display import display\n",
        "def color_negative_red(val):\n",
        "    color = 'red' if val < 0.5 else 'green'\n",
        "    return 'color: %s' % color\n",
        "\n",
        "df_grid_aug = df_grid_aug.style.applymap(color_negative_red, subset=['val_recall', 'val_precision', 'val_accuracy', 'val_auc', 'val_loss', 'val_f1'])\n",
        "display(df_grid_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_grid_aug.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_grid_aug = df_grid_aug[['rotation_range', 'width_shift_range', 'height_shift_range', 'zoom_range', 'horizontal_flip', 'vertical_flip', 'val_recall', 'val_precision', 'val_accuracy', 'val_auc', 'val_loss', 'val_f1']]\n",
        "df_grid_aug.sort_values(by=['val_f1'], ascending=False, inplace=True)\n",
        "print(df_grid_aug.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#export the dataframe to csv\n",
        "df_grid_aug.to_csv(FILEPATH_OUTPUT+\"df_grid_aug.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define df_grid as df_grid_aug\n",
        "df_grid = df_grid_aug\n",
        "\n",
        "# Plot all validation metrics for each parameter combination in one figure grid of 3x2\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Lineplot the validation recall and train recall\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='val_recall', label='val_recall')\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='recall', label='train_recall')\n",
        "plt.title('Recall')\n",
        "plt.xlabel('rotation_range')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Lineplot the validation precision and train precision\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='val_precision', label='val_precision')\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='precision', label='train_precision')\n",
        "plt.title('Precision')\n",
        "plt.xlabel('rotation_range')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Lineplot the validation accuracy and train accuracy\n",
        "plt.subplot(2, 3, 3)\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='val_accuracy', label='val_accuracy')\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='accuracy', label='train_accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('rotation_range')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Lineplot the validation auc and train auc\n",
        "plt.subplot(2, 3, 4)\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='val_auc', label='val_auc')\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='auc', label='train_auc')\n",
        "plt.title('AUC')\n",
        "plt.xlabel('rotation_range')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Lineplot the validation loss and train loss\n",
        "plt.subplot(2, 3, 5)\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='val_loss', label='val_loss')\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='loss', label='train_loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('rotation_range')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Lineplot the validation f1 and train f1\n",
        "plt.subplot(2, 3, 6)\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='val_f1', label='val_f1')\n",
        "sns.lineplot(data=df_grid, x='rotation_range', y='f1', label='train_f1')\n",
        "plt.title('F1')\n",
        "plt.xlabel('rotation_range')\n",
        "plt.ylabel('F1')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "#add the fixed learning rate to the grid title and batch size and Model technique\n",
        "plt.suptitle(f\"Learning rate: {learning_rate}, batch size: {batch_size}, MobilNetV3Large_pretrained-weights_Partially_fixed-layers_custom-conv2D\")\n",
        "\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show which grid params are the best for val_recall, val_precision, val_accuracy, val_auc, val_loss, f1\n",
        "# Sort the dataframe by the validation recall\n",
        "df_grid.sort_values(by='val_recall', ascending=False, inplace=True)\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(df_grid.head())\n",
        "\n",
        "# Sort the dataframe by the validation precision\n",
        "df_grid.sort_values(by='val_precision', ascending=False, inplace=True)\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(df_grid.head())\n",
        "\n",
        "# Sort the dataframe by the validation accuracy\n",
        "df_grid.sort_values(by='val_accuracy', ascending=False, inplace=True)\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(df_grid.head())\n",
        "\n",
        "# Sort the dataframe by the validation auc\n",
        "df_grid.sort_values(by='val_auc', ascending=False, inplace=True)\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(df_grid.head())\n",
        "\n",
        "# Sort the dataframe by the validation loss\n",
        "df_grid.sort_values(by='val_loss', ascending=True, inplace=True)\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(df_grid.head())\n",
        "\n",
        "# Sort the dataframe by the validation f1\n",
        "df_grid.sort_values(by='val_f1', ascending=False, inplace=True)\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(df_grid.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scatterplot val_AUC and val_recall for all grid params from df_grid with a for loop\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Scatterplot the validation AUC and validation recall\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.scatterplot(data=df_grid, x='val_auc', y='val_recall', hue='rotation_range')\n",
        "plt.title('AUC vs. Recall')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Scatterplot the validation AUC and validation precision\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.scatterplot(data=df_grid, x='val_auc', y='val_precision', hue='rotation_range')\n",
        "plt.title('AUC vs. Precision')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Scatterplot the validation AUC and validation accuracy\n",
        "plt.subplot(2, 3, 3)\n",
        "\n",
        "sns.scatterplot(data=df_grid, x='val_auc', y='val_accuracy', hue='rotation_range')\n",
        "plt.title('AUC vs. Accuracy')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Scatterplot the validation AUC and validation loss\n",
        "plt.subplot(2, 3, 4)\n",
        "sns.scatterplot(data=df_grid, x='val_auc', y='val_loss', hue='rotation_range')\n",
        "plt.title('AUC vs. Loss')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Scatterplot the validation AUC and validation f1\n",
        "plt.subplot(2, 3, 5)\n",
        "sns.scatterplot(data=df_grid, x='val_auc', y='val_f1', hue='rotation_range')\n",
        "plt.title('AUC vs. F1')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('F1')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# empty subplot\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.axis('off')\n",
        "\n",
        "# Add the fixed learning rate to the grid title and batch size and Model technique\n",
        "plt.suptitle(f\"Learning rate: {learning_rate}, batch size: {batch_size}, MobilNetV3Large\")\n",
        "\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "timestamp = datetime.now()\n",
        "model_path = f\"{FILEPATH_OUTPUT}model_bjzim_MobileNetV3Large{timestamp}.h5\"\n",
        "model.save(model_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
