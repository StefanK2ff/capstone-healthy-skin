{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Importiere benötigte Bibliotheken ----\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tensorflow.keras import layers, Model, regularizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV3Large\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
        "from tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D, Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, recall_score, precision_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaSgmHzMsoh"
      },
      "outputs": [],
      "source": [
        "# ---- Setze Konstanten und Parameter ----\n",
        "SEED = 42\n",
        "NUM_EPOCHS = 15\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = (224, 224)\n",
        "TARGET_LABEL = \"dx\"\n",
        "BALANCE_LABEL = \"dx\"\n",
        "FILEPATH_JPGS = './../data/jpgs/'\n",
        "FILEPATH_PROCESSED = './../data/processed/'\n",
        "FILEPATH_OUTPUT = './../data/bjzim-models/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Initialisiere sonstige Variablen ----\n",
        "pbar = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClM_Z44mMumW"
      },
      "outputs": [],
      "source": [
        "# ---- Dateipfade und Set-Namen ----\n",
        "filepaths = [\n",
        "    (\"Trainingsset\", FILEPATH_PROCESSED + \"train_from_Metadata_processed.csv\"),\n",
        "    (\"Validierungsset\", FILEPATH_PROCESSED + \"validation_from_Metadata_processed.csv\"),\n",
        "    (\"Testset\", FILEPATH_PROCESSED + \"test_from_Metadata_processed.csv\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Funktion zur Überprüfung von augmentierten Daten ----\n",
        "def check_augmented_data(df, set_name):\n",
        "    if df['image_id'].str.startswith('aug_').any():\n",
        "        print(f\"Warnung: Augmentierte Daten im {set_name} gefunden.\")\n",
        "\n",
        "# ---- Überprüfung ----\n",
        "for set_name, filepath in filepaths:\n",
        "    df = pd.read_csv(filepath)\n",
        "    check_augmented_data(df, set_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(FILEPATH_PROCESSED+\"train_from_Metadata_processed.csv\")\n",
        "validation_df = pd.read_csv(FILEPATH_PROCESSED+\"validation_from_Metadata_processed.csv\")\n",
        "test_df = pd.read_csv(FILEPATH_PROCESSED+\"test_from_Metadata_processed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your augmentation parameters\n",
        "\n",
        "\n",
        "aug_params_recall = {\n",
        "    'height_shift_range': 0.1,\n",
        "    'horizontal_flip': True,\n",
        "    'rotation_range': 0,\n",
        "    'vertical_flip': True,\n",
        "    'width_shift_range': 0.2,\n",
        "    'zoom_range': 0.05\n",
        "}\n",
        "\n",
        "# aug_params_recall = {\n",
        "#     'height_shift_range': 0.05,\n",
        "#     'horizontal_flip': False,\n",
        "#     'rotation_range': 30,\n",
        "#     'vertical_flip': True,\n",
        "#     'width_shift_range': 0.2,\n",
        "#     'zoom_range': 0.05\n",
        "# }\n",
        "# Create a grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'conv2d_filters': [128],\n",
        "    'dense_units': [64, 128],\n",
        "    'dropout_rate': [0.5, 0.7],\n",
        "    'batch_size': [64],\n",
        "    'optimizer': ['Adam'],\n",
        "    'weight_regularization': ['l2']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen_train = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=aug_params_recall['rotation_range'],\n",
        "    width_shift_range=aug_params_recall['width_shift_range'],\n",
        "    height_shift_range=aug_params_recall['height_shift_range'],\n",
        "    zoom_range=aug_params_recall['zoom_range'],\n",
        "    horizontal_flip=aug_params_recall['horizontal_flip'],\n",
        "    vertical_flip=aug_params_recall['vertical_flip'],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_data_generator = datagen_train.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=FILEPATH_JPGS,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=TARGET_LABEL,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "datagen_validation = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0, #see above\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "validation_generator = datagen_validation.flow_from_dataframe(\n",
        "    dataframe=validation_df,\n",
        "    directory=FILEPATH_JPGS,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=TARGET_LABEL,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_f1',\n",
        "    mode='max',\n",
        "    patience=8, #20,15,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_f1',\n",
        "    mode='max',\n",
        "    factor=0.5, #0.1,\n",
        "    patience=3, #12,8,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'model_best_weights.h5', \n",
        "    save_best_only=True, \n",
        "    save_weights_only=True, \n",
        "    monitor='val_f1', \n",
        "    mode='max', \n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mit GlobalAveragepooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomMetrics(Callback):\n",
        "    def __init__(self, validation_generator):\n",
        "        super().__init__()\n",
        "        self.validation_generator = validation_generator\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        val_preds = np.argmax(self.model.predict(self.validation_generator), axis=1)\n",
        "        val_true = self.validation_generator.classes\n",
        "        val_recall = recall_score(val_true, val_preds, average='weighted')\n",
        "        val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
        "        val_auc = roc_auc_score(val_true, self.model.predict(self.validation_generator), multi_class='ovr', average='weighted')\n",
        "        logs['val_recall'] = val_recall\n",
        "        logs['val_f1'] = val_f1\n",
        "        logs['val_auc'] = val_auc\n",
        "        print(f\" - val_recall: {val_recall: .5f} - val_f1: {val_f1: .5f} - val_auc: {val_auc: .5f}\")\n",
        "        print(\"-----------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_evaluate_model(params, train_df, validation_df, FILEPATH_JPGS, TARGET_LABEL, IMAGE_SIZE, BATCH_SIZE):\n",
        "\n",
        "    base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "    # Unfreeze some of the 269 layers for fine-tuning\n",
        "    for layer in base_model.layers[:150]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[150:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Regularization\n",
        "    reg_type = params.get('weight_regularization', None)\n",
        "    if reg_type == 'l1':\n",
        "        reg = regularizers.l1(0.01)\n",
        "    elif reg_type == 'l2':\n",
        "        reg = regularizers.l2(0.01)\n",
        "    else:\n",
        "        reg = None\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = layers.Dense(params['dense_units'], activation='relu', kernel_regularizer=reg)(x)\n",
        "    x = layers.Dropout(params['dropout_rate'])(x)\n",
        "    x = layers.Dense(params['dense_units'] // 2, activation='relu', kernel_regularizer=reg)(x)  # Zusätzlicher Dense-Layer\n",
        "    x = layers.Dropout(params['dropout_rate'])(x)  # Zusätzlicher Dropout-Layer\n",
        "    x = layers.Dense(7, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=params['learning_rate']), \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Define data generators with augmentation\n",
        "    datagen_train = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rotation_range=aug_params_recall['rotation_range'],\n",
        "        width_shift_range=aug_params_recall['width_shift_range'],\n",
        "        height_shift_range=aug_params_recall['height_shift_range'],\n",
        "        zoom_range=aug_params_recall['zoom_range'],\n",
        "        horizontal_flip=aug_params_recall['horizontal_flip'],\n",
        "        vertical_flip=aug_params_recall['vertical_flip'],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    datagen_validation = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        preprocessing_function=preprocess_input\n",
        "    )\n",
        "\n",
        "    train_generator = datagen_train.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=FILEPATH_JPGS,\n",
        "        x_col=\"image_id\",\n",
        "        y_col=TARGET_LABEL,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=True,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen_validation.flow_from_dataframe(\n",
        "        dataframe=validation_df,\n",
        "        directory=FILEPATH_JPGS,\n",
        "        x_col=\"image_id\",\n",
        "        y_col=TARGET_LABEL,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=False,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    custom_metrics = CustomMetrics(validation_generator=validation_generator)\n",
        "\n",
        "    # Train the model with callbacks\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=validation_generator,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks=[custom_metrics, early_stopping, reduce_lr, model_checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "\n",
        "    # Calculate F1 score\n",
        "    val_preds = model.predict(validation_generator)\n",
        "    val_true_labels = validation_generator.labels  # Änderung hier\n",
        "    val_pred_labels = np.argmax(val_preds, axis=1)  # Neue Zeile\n",
        "    f1 = f1_score(val_true_labels, val_pred_labels, average='weighted')\n",
        "\n",
        "    return model, f1, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize variables\n",
        "results_df = pd.DataFrame(columns=['learning_rate', 'conv2d_filters', 'dense_units', 'dropout_rate', 'val_f1'])\n",
        "best_models, completed_iterations = [], 0\n",
        "total_iterations = len(ParameterGrid(param_grid))\n",
        "\n",
        "def print_remaining_iterations(total, completed):\n",
        "    return f\"{total - completed} iterations remaining\"\n",
        "\n",
        "# Main loop for hyperparameter tuning\n",
        "pbar = tqdm(total=total_iterations, desc=\"Hyperparameter Optimization\")\n",
        "for params in tqdm(ParameterGrid(param_grid), total=total_iterations, desc=\"Hyperparameter Optimization\"):\n",
        "    model, f1, _ = train_evaluate_model(params, train_df, validation_df, FILEPATH_JPGS, TARGET_LABEL, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "    # Update results DataFrame and best models list\n",
        "    results_df = results_df.append({**params, 'val_f1': f1}, ignore_index=True)\n",
        "    best_models = sorted(best_models + [(f1, model)], key=lambda x: x[0], reverse=True)[:5]\n",
        "    \n",
        "    # Progress update\n",
        "    completed_iterations += 1\n",
        "    print(print_remaining_iterations(total_iterations, completed_iterations))\n",
        "    pbar.update(1)\n",
        "pbar.close()\n",
        "\n",
        "# Save the top 5 models and results DataFrame\n",
        "for i, (f1, model) in enumerate(best_models):\n",
        "    model.save(f'best_model_{i + 1}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.h5')\n",
        "    print(f\"Saved best_model_{i + 1} with F1: {f1}\")\n",
        "    \n",
        "results_df.to_csv(FILEPATH_OUTPUT + 'hyperparameter_tuning_results.csv', index=False)\n",
        "\n",
        "print(\"Best F1 Scores:\", [f1 for f1, _ in best_models])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show me results sorted by val_f1\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(model.layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotte die Lernkurven\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotte die Genauigkeit\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plotte den Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
