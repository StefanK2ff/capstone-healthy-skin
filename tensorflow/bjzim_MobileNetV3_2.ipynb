{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_6ed-mPEi4-",
        "outputId": "d61f0708-4fac-4dbf-b404-a1da9ae3ef0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (1.4.4)\n",
            "Requirement already satisfied: keras-core in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-tuner) (0.1.7)\n",
            "Requirement already satisfied: packaging in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-core->keras-tuner) (2.0.0)\n",
            "Requirement already satisfied: numpy in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-core->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: rich in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-core->keras-tuner) (13.6.0)\n",
            "Requirement already satisfied: namex in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-core->keras-tuner) (0.0.7)\n",
            "Requirement already satisfied: h5py in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-core->keras-tuner) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from keras-core->keras-tuner) (0.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from requests->keras-tuner) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from rich->keras-core->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from rich->keras-core->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/bjz/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-tuner) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAMd5hCqEqSv",
        "outputId": "ea6104ac-a9b6-4237-f172-2210c7e6e8a4"
      },
      "outputs": [],
      "source": [
        "from kerastuner.tuners import RandomSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "DD-JKEZMLbeb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV3Large\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import metrics  # Import metrics\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "scaSgmHzMsoh"
      },
      "outputs": [],
      "source": [
        "# Konstanten und Einstellungen\n",
        "SEED = 421\n",
        "NUM_EPOCHS = 30\n",
        "TARGET_LABEL = \"dx_binary\"\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 64\n",
        "FILEPATH_JPGS = './../data/jpgs/'\n",
        "FILEPATH_PROCESSED=\"./../data/processed/\"\n",
        "FILEPATH_OUTPUT = './../data/jpgs/'  # Replace with your folder path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ClM_Z44mMumW"
      },
      "outputs": [],
      "source": [
        "# Daten lesen\n",
        "train_df = pd.read_csv(FILEPATH_PROCESSED + \"train_from_Metadata_processed.csv\")\n",
        "validation_df = pd.read_csv(FILEPATH_PROCESSED + \"validation_from_Metadata_processed.csv\")\n",
        "test_df = pd.read_csv(FILEPATH_PROCESSED + \"test_from_Metadata_processed.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "_dpfm34IM7cs",
        "outputId": "ea8f259b-8af4-4bab-a16e-29b05ea5c68f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "      <th>dataset</th>\n",
              "      <th>dx_binary</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAM_0002681</td>\n",
              "      <td>ISIC_0029381.jpg</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>30.0</td>\n",
              "      <td>male</td>\n",
              "      <td>back</td>\n",
              "      <td>vidir_molemax</td>\n",
              "      <td>not_skin_cancer</td>\n",
              "      <td>./../data/jpgs/ISIC_0029381.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAM_0005320</td>\n",
              "      <td>ISIC_0025356.jpg</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>35.0</td>\n",
              "      <td>female</td>\n",
              "      <td>back</td>\n",
              "      <td>vidir_molemax</td>\n",
              "      <td>not_skin_cancer</td>\n",
              "      <td>./../data/jpgs/ISIC_0025356.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HAM_0003724</td>\n",
              "      <td>ISIC_0025036.jpg</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>50.0</td>\n",
              "      <td>female</td>\n",
              "      <td>abdomen</td>\n",
              "      <td>vidir_molemax</td>\n",
              "      <td>not_skin_cancer</td>\n",
              "      <td>./../data/jpgs/ISIC_0025036.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HAM_0006809</td>\n",
              "      <td>ISIC_0031690.jpg</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>40.0</td>\n",
              "      <td>male</td>\n",
              "      <td>abdomen</td>\n",
              "      <td>vidir_molemax</td>\n",
              "      <td>not_skin_cancer</td>\n",
              "      <td>./../data/jpgs/ISIC_0031690.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAM_0003443</td>\n",
              "      <td>ISIC_0032485.jpg</td>\n",
              "      <td>nv</td>\n",
              "      <td>follow_up</td>\n",
              "      <td>35.0</td>\n",
              "      <td>female</td>\n",
              "      <td>abdomen</td>\n",
              "      <td>vidir_molemax</td>\n",
              "      <td>not_skin_cancer</td>\n",
              "      <td>./../data/jpgs/ISIC_0032485.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lesion_id          image_id  dx    dx_type   age     sex localization  \\\n",
              "0  HAM_0002681  ISIC_0029381.jpg  nv  follow_up  30.0    male         back   \n",
              "1  HAM_0005320  ISIC_0025356.jpg  nv  follow_up  35.0  female         back   \n",
              "2  HAM_0003724  ISIC_0025036.jpg  nv  follow_up  50.0  female      abdomen   \n",
              "3  HAM_0006809  ISIC_0031690.jpg  nv  follow_up  40.0    male      abdomen   \n",
              "4  HAM_0003443  ISIC_0032485.jpg  nv  follow_up  35.0  female      abdomen   \n",
              "\n",
              "         dataset        dx_binary                       image_path  \n",
              "0  vidir_molemax  not_skin_cancer  ./../data/jpgs/ISIC_0029381.jpg  \n",
              "1  vidir_molemax  not_skin_cancer  ./../data/jpgs/ISIC_0025356.jpg  \n",
              "2  vidir_molemax  not_skin_cancer  ./../data/jpgs/ISIC_0025036.jpg  \n",
              "3  vidir_molemax  not_skin_cancer  ./../data/jpgs/ISIC_0031690.jpg  \n",
              "4  vidir_molemax  not_skin_cancer  ./../data/jpgs/ISIC_0032485.jpg  "
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6NALLdLlMbOW"
      },
      "outputs": [],
      "source": [
        "def center_crop_image(np_image: np.ndarray, target_size: tuple = (224, 224)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Center crop an image to a square and resize it to the target size.\n",
        "\n",
        "    Args:\n",
        "        np_image (np.ndarray): Image to be cropped in numpy array format.\n",
        "        target_size (tuple): The target size to resize the cropped image, default is (224, 224).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Cropped and resized image in numpy array format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the scaled numpy array to a PIL Image with original pixel values (0-255)\n",
        "    image = Image.fromarray((np_image * 255).astype(np.uint8))\n",
        "\n",
        "    # Get dimensions\n",
        "    width, height = image.size\n",
        "\n",
        "    # Calculate the dimensions of the cropped area (choose the shorter side)\n",
        "    new_dimension = min(width, height)\n",
        "\n",
        "    # Calculate cropping box\n",
        "    left = (width - new_dimension) / 2\n",
        "    top = (height - new_dimension) / 2\n",
        "    right = (width + new_dimension) / 2\n",
        "    bottom = (height + new_dimension) / 2\n",
        "\n",
        "    # Crop and resize\n",
        "    image = image.crop((left, top, right, bottom)).resize(target_size)\n",
        "\n",
        "    # Convert back to scaled numpy array (0-1)\n",
        "    np_image = np.array(image) / 255.0\n",
        "\n",
        "    return np_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "lajwzsTFNyIl"
      },
      "outputs": [],
      "source": [
        "def resize_as_preprocess(np_image: np.ndarray, image_size: tuple) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Resize an image to the target size.\n",
        "\n",
        "    Args:\n",
        "        np_image (np.ndarray): The image to be resized, in numpy array format.\n",
        "        image_size (tuple): The target size to resize the image to, format (width, height).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The resized image in numpy array format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the scaled numpy array (0-1) to a PIL Image with original pixel values (0-255)\n",
        "    image = Image.fromarray((np_image * 255).astype(np.uint8))\n",
        "\n",
        "    # Resize the image\n",
        "    # image = image.resize(image_size, Image.ANTIALIAS) This is old\n",
        "    image = image.resize(image_size, Image.LANCZOS)\n",
        "\n",
        "\n",
        "    # Convert back to a scaled numpy array (0-1)\n",
        "    np_image = np.array(image) / 255.0\n",
        "\n",
        "    return np_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "4R6vslGIN_uq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "\n",
        "def custom_preprocessing(np_image: np.ndarray, image_size: tuple) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Custom preprocessing function that combines center cropping, resizing, and MobileNetV3Large's preprocess_input.\n",
        "\n",
        "    Args:\n",
        "        np_image (np.ndarray): The image to be preprocessed, in numpy array format.\n",
        "        image_size (tuple): The target size to resize the image to, format (width, height).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The preprocessed image in numpy array format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Center crop the image to square format\n",
        "    np_image = center_crop_image(np_image)\n",
        "\n",
        "    # Step 2: Resize the image to the target dimensions\n",
        "    np_image = resize_as_preprocess(np_image, image_size)\n",
        "\n",
        "    # Step 3: Apply MobileNetV3Large's preprocess_input function\n",
        "    np_image = preprocess_input(np_image)\n",
        "\n",
        "    return np_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "vuvdpiytOU6N"
      },
      "outputs": [],
      "source": [
        "def wrapped_custom_preprocessing(x):\n",
        "    return custom_preprocessing(x, IMAGE_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ZrRGRb9OOl5P"
      },
      "outputs": [],
      "source": [
        "# Setting up the Image Data Generator for the train data set\n",
        "datagen_train = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,  # Rescale pixel values to [0, 1], important for neural networks\n",
        "    preprocessing_function=wrapped_custom_preprocessing  # Apply custom preprocessing\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zD3n9BtCOiDi"
      },
      "outputs": [],
      "source": [
        "# Setting up the Image Data Generator for the validation data set\n",
        "datagen_validation = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,  # Rescale pixel values to [0, 1], important for neural networks\n",
        "    preprocessing_function=wrapped_custom_preprocessing  # Apply custom preprocessing\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5iLdYVyO512",
        "outputId": "7e74b069-4c45-4008-e611-180852e28ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2003 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the validation data generator using flow_from_dataframe method\n",
        "validation_generator = datagen_validation.flow_from_dataframe(\n",
        "    dataframe=validation_df,            # DataFrame containing the filepaths and labels\n",
        "    directory=FILEPATH_JPGS,            # Path to the directory to read images from\n",
        "    x_col=\"image_id\",                   # Column in DataFrame containing the filepaths\n",
        "    y_col=TARGET_LABEL,                 # Column in DataFrame containing the labels\n",
        "    class_mode=\"categorical\",           # Mode for yielding the labels (categorical for multi-class problems)\n",
        "    target_size=IMAGE_SIZE,             # Target size for resizing the images\n",
        "    batch_size=BATCH_SIZE               # Number of images to load at each iteration\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-fcOtbUPKSF",
        "outputId": "10c3d7bc-5caf-4f57-a72e-7b0c04e1f916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5256 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the training data generator using the flow_from_dataframe method\n",
        "train_data_generator = datagen_train.flow_from_dataframe(\n",
        "    dataframe=train_df,                 # DataFrame containing the filepaths and labels\n",
        "    directory=FILEPATH_JPGS,            # Path to the directory to read images from\n",
        "    x_col=\"image_id\",                   # Column in DataFrame containing the filepaths\n",
        "    y_col=TARGET_LABEL,                 # Column in DataFrame containing the labels\n",
        "    class_mode=\"categorical\",           # Mode for yielding the labels (categorical for multi-class problems)\n",
        "    target_size=IMAGE_SIZE,             # Target size for resizing the images\n",
        "    batch_size=BATCH_SIZE               # Number of images to load at each iteration\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd5qtmFQcIRB"
      },
      "source": [
        "**Build Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "hIhFCOFSPrKg"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    # Hyperparameter\n",
        "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    dense_units = hp.Int('dense_units', min_value=32, max_value=512, step=32)\n",
        "\n",
        "    # MobileNetV3Large Modell mit eingefrorenen Gewichtungen\n",
        "    base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Anpassbare Schichten oben\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(dense_units, activation='relu')(x)\n",
        "    predictions = layers.Dense(train_df[TARGET_LABEL].nunique(), activation='softmax')(x)\n",
        "\n",
        "    # Gesamtmodell erstellen\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Modell kompilieren\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', metrics.Recall(), metrics.Precision(),\n",
        "                           tf.keras.metrics.AUC(curve='PR', name='f1_score')])\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuDvjIp-GdEx",
        "outputId": "354473a4-a349-4c9c-e419-d08d241f4b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 01m 55s]\n",
            "val_accuracy: 0.8047928214073181\n",
            "\n",
            "Best val_accuracy So Far: 0.8047928214073181\n",
            "Total elapsed time: 00h 05m 31s\n"
          ]
        }
      ],
      "source": [
        "from kerastuner.tuners import BayesianOptimization\n",
        "\n",
        "# Bayesian Optimization\n",
        "tuner = BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,  # Reduzierte Anzahl der Trials\n",
        "    num_initial_points=2,  # Anzahl der Randomisierungspunkte vor Beginn der Optimierung\n",
        "    directory='\"./../models/bjzim/\"',\n",
        "    project_name='MobileNetV3_HAM10000_Tuning_binary'\n",
        ")\n",
        "\n",
        "# Reduziere die Anzahl der Epochen\n",
        "tuner.search(train_data_generator,\n",
        "             epochs=10,  # Reduzierte Epochen\n",
        "             validation_data=validation_generator,\n",
        "             callbacks=[EarlyStopping(monitor='val_accuracy', patience=1)]  # Reduzierte Geduld\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ZAEIV4-LGuQy"
      },
      "outputs": [],
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "qQCMJB9uGzzi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "model = tuner.hypermodel.build(best_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "4eogoCTsaYKJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Definieren des PlotLearning Callbacks\n",
        "class PlotLearning(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.metrics = {}\n",
        "        if 'metrics' in self.params:\n",
        "            for metric in self.params['metrics']:\n",
        "                self.metrics[metric] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        for metric in self.params.get('metrics', []):\n",
        "            if metric in logs:\n",
        "                self.metrics[metric].append(logs[metric])\n",
        "\n",
        "        # Plot the metrics\n",
        "        f, ax = plt.subplots(1, len(self.metrics), figsize=(20, 5))\n",
        "        f.suptitle('Epoch {}'.format(epoch))\n",
        "        for i, metric in enumerate(self.metrics.keys()):\n",
        "            ax[i].plot(self.metrics[metric])\n",
        "            ax[i].set_title(metric)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "_5MpSwYvQs9B"
      },
      "outputs": [],
      "source": [
        "plot_learning = PlotLearning()\n",
        "\n",
        "# Aktualisierte Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_recall_10', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_recall_10', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "callbacks_list = [early_stopping, reduce_lr, plot_learning]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "SoJ1dy7t_S6k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Initialisiere den Adam-Optimizer mit einer benutzerdefinierten Lernrate\n",
        "optimizer = Adam(learning_rate=0.001)  # Du kannst die Lernrate nach Bedarf anpassen\n",
        "\n",
        "# Kompilieren des Modells\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', 'Recall', 'Precision',  # oder die genauen Namen der Metriken, wenn du benutzerdefinierte Metriken verwendest\n",
        "                       tf.keras.metrics.AUC(curve='PR', name='f1_score')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtiS9NWTNkH2",
        "outputId": "545f599a-75a1-47bf-9347-133f22f9f4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "83/83 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.5497 - recall: 0.5497 - precision: 0.5497 - f1_score: 0.5447WARNING:tensorflow:Early stopping conditioned on metric `val_recall_10` which is not available. Available metrics are: loss,accuracy,recall,precision,f1_score,val_loss,val_accuracy,val_recall,val_precision,val_f1_score\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_recall_10` which is not available. Available metrics are: loss,accuracy,recall,precision,f1_score,val_loss,val_accuracy,val_recall,val_precision,val_f1_score\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_recall_10` which is not available. Available metrics are: loss,accuracy,recall,precision,f1_score,val_loss,val_accuracy,val_recall,val_precision,val_f1_score,lr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_recall_10` which is not available. Available metrics are: loss,accuracy,recall,precision,f1_score,val_loss,val_accuracy,val_recall,val_precision,val_f1_score,lr\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Number of columns must be a positive integer, not 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Trainingsaufruf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_data_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks_list,  \u001b[39m# Verwende Callbacks f端r fr端hzeitiges Anhalten und Reduzieren der Lernrate\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     workers\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m  \u001b[39m# Aktiviere dies, wenn du einen Generator parallel verwendest\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
            "File \u001b[0;32m~/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics[metric]\u001b[39m.\u001b[39mappend(logs[metric])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Plot the metrics\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m f, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49msubplots(\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetrics), figsize\u001b[39m=\u001b[39;49m(\u001b[39m20\u001b[39;49m, \u001b[39m5\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m f\u001b[39m.\u001b[39msuptitle(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bjz/neueFische/capstone/capstone-healthy-skin/tensorflow/bjzim_MobileNetV3_2.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, metric \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mkeys()):\n",
            "File \u001b[0;32m~/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:1502\u001b[0m, in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[39mCreate a figure and a set of subplots.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \n\u001b[1;32m   1500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m fig \u001b[39m=\u001b[39m figure(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfig_kw)\n\u001b[0;32m-> 1502\u001b[0m axs \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49msubplots(nrows\u001b[39m=\u001b[39;49mnrows, ncols\u001b[39m=\u001b[39;49mncols, sharex\u001b[39m=\u001b[39;49msharex, sharey\u001b[39m=\u001b[39;49msharey,\n\u001b[1;32m   1503\u001b[0m                    squeeze\u001b[39m=\u001b[39;49msqueeze, subplot_kw\u001b[39m=\u001b[39;49msubplot_kw,\n\u001b[1;32m   1504\u001b[0m                    gridspec_kw\u001b[39m=\u001b[39;49mgridspec_kw, height_ratios\u001b[39m=\u001b[39;49mheight_ratios,\n\u001b[1;32m   1505\u001b[0m                    width_ratios\u001b[39m=\u001b[39;49mwidth_ratios)\n\u001b[1;32m   1506\u001b[0m \u001b[39mreturn\u001b[39;00m fig, axs\n",
            "File \u001b[0;32m~/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages/matplotlib/figure.py:905\u001b[0m, in \u001b[0;36mFigureBase.subplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwidth_ratios\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must not be defined both as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mparameter and as key in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mgridspec_kw\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    903\u001b[0m     gridspec_kw[\u001b[39m'\u001b[39m\u001b[39mwidth_ratios\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m width_ratios\n\u001b[0;32m--> 905\u001b[0m gs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_gridspec(nrows, ncols, figure\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgridspec_kw)\n\u001b[1;32m    906\u001b[0m axs \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39msubplots(sharex\u001b[39m=\u001b[39msharex, sharey\u001b[39m=\u001b[39msharey, squeeze\u001b[39m=\u001b[39msqueeze,\n\u001b[1;32m    907\u001b[0m                   subplot_kw\u001b[39m=\u001b[39msubplot_kw)\n\u001b[1;32m    908\u001b[0m \u001b[39mreturn\u001b[39;00m axs\n",
            "File \u001b[0;32m~/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages/matplotlib/figure.py:1527\u001b[0m, in \u001b[0;36mFigureBase.add_gridspec\u001b[0;34m(self, nrows, ncols, **kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[39mReturn a `.GridSpec` that has this figure as a parent.  This allows\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[39mcomplex layout of Axes in the figure.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \n\u001b[1;32m   1524\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# pop in case user has added this...\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m gs \u001b[39m=\u001b[39m GridSpec(nrows\u001b[39m=\u001b[39;49mnrows, ncols\u001b[39m=\u001b[39;49mncols, figure\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1528\u001b[0m \u001b[39mreturn\u001b[39;00m gs\n",
            "File \u001b[0;32m~/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages/matplotlib/gridspec.py:379\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhspace \u001b[39m=\u001b[39m hspace\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39m=\u001b[39m figure\n\u001b[0;32m--> 379\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(nrows, ncols,\n\u001b[1;32m    380\u001b[0m                  width_ratios\u001b[39m=\u001b[39;49mwidth_ratios,\n\u001b[1;32m    381\u001b[0m                  height_ratios\u001b[39m=\u001b[39;49mheight_ratios)\n",
            "File \u001b[0;32m~/neueFische/capstone/capstone-healthy-skin/.venv/lib/python3.11/site-packages/matplotlib/gridspec.py:52\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     50\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of rows must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mnrows\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ncols, Integral) \u001b[39mor\u001b[39;00m ncols \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of columns must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mncols\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nrows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ncols \u001b[39m=\u001b[39m nrows, ncols\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_height_ratios(height_ratios)\n",
            "\u001b[0;31mValueError\u001b[0m: Number of columns must be a positive integer, not 0"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Trainingsaufruf\n",
        "history = model.fit(\n",
        "    train_data_generator,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    verbose=1,\n",
        "    validation_data=validation_generator,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks_list,  # Verwende Callbacks f端r fr端hzeitiges Anhalten und Reduzieren der Lernrate\n",
        "    workers=-1,\n",
        "    use_multiprocessing=True  # Aktiviere dies, wenn du einen Generator parallel verwendest\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv28nv0YItGT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_generator = datagen_test.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=FILEPATH_JPGS,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=TARGET_LABEL,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Get the true labels and predicted labels\n",
        "true_labels = test_generator.classes\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate and print the overall accuracy\n",
        "overall_accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy of the network on the test images: {overall_accuracy * 100:.2f} %\")\n",
        "\n",
        "# Calculate and print class-wise accuracies\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_labels)\n",
        "print(\"Class-wise evaluation:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72yveiV2Nu1Y"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Initialize and compile ResNet model\n",
        "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "x = resnet_base.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "predictions = layers.Dense(train_df[TARGET_LABEL].nunique(), activation='softmax')(x)\n",
        "resnet_model = Model(inputs=resnet_base.input, outputs=predictions)\n",
        "\n",
        "resnet_model.compile(optimizer='adam',\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# Train ResNet model\n",
        "resnet_history = resnet_model.fit(train_data_generator, epochs=NUM_EPOCHS, validation_data=validation_data_generator, callbacks=callbacks_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkywldvON4Ia"
      },
      "outputs": [],
      "source": [
        "# Get predictions from both models\n",
        "mobilenet_preds = history.predict(validation_data_generator)\n",
        "resnet_preds = resnet_model.predict(validation_data_generator)\n",
        "\n",
        "# Average the predictions\n",
        "avg_preds = (mobilenet_preds + resnet_preds) / 2\n",
        "\n",
        "# Convert averaged predictions to class labels\n",
        "final_preds = np.argmax(avg_preds, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQdf7b08OKzX"
      },
      "outputs": [],
      "source": [
        "# Extract ground truth labels from validation data generator\n",
        "ground_truth = validation_data_generator.classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXc6wVEzONq5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Convert averaged predictions to class labels\n",
        "final_preds = np.argmax(avg_preds, axis=1)\n",
        "\n",
        "# Compute overall accuracy\n",
        "ensemble_accuracy = accuracy_score(ground_truth, final_preds)\n",
        "print(f'Ensemble Accuracy: {ensemble_accuracy * 100:.2f}%')\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score for each class\n",
        "report = classification_report(ground_truth, final_preds, target_names=validation_data_generator.class_indices.keys())\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
