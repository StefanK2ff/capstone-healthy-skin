{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod. Image Loader with Albumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This notebook loads images from a directory, and handles imbalanced classes with down/re/upsampling and augmentation.\n",
    "\n",
    "This notebook should be run from the top to the bottom, resulting in 3 files:\n",
    "\n",
    "* `train_from_Metadata_processed.csv` - a csv file with the training data\n",
    "* `validation_from_Metadata_processed.csv` - a csv file with the validation data\n",
    "* `test_from_Metadata_processed.csv` - a csv file with the test data\n",
    "\n",
    "These files encode the images with labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and inital setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "from albumentations import (Compose, RandomCrop, Normalize, HorizontalFlip, Resize, RandomBrightnessContrast, CoarseDropout, GridDistortion, HueSaturationValue, GaussianBlur, Rotate, RandomResizedCrop)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from helperfunctions import imagehelper as ih\n",
    "\n",
    "SEED = 4932\n",
    "MAX_SAMPLES = 750\n",
    "\n",
    "# File path variables\n",
    "# please make sure to use the correct path to the meta data file\n",
    "FILEPATH_JPGS = './../data/jpgs/'\n",
    "FILEPATH_METADATA=\"./../data/processed/Metadata_processed.csv\"\n",
    "FILEPATH_OUTPUT = './../data/jpgs/' \n",
    "\n",
    "TARGET_LABEL=\"dx\"    # Needed for test train split\n",
    "BALANCE_LABEL=\"dx\"          # Needed for balancing the dataset\n",
    "IMAGE_SIZE = (224, 224)     # Adapt to your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting all previously augmented images in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bash script as a string\n",
    "bash_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "output_folder=\"./../data/jpgs/\" \n",
    "\n",
    "# Delete images with the \"aug_\" prefix\n",
    "find \"$output_folder\" -type f -name \"aug_*\" -delete\n",
    "\n",
    "echo \"Deleted augmented images with 'aug_' prefix in $output_folder\"\n",
    "\"\"\"\n",
    "\n",
    "# Save the bash script to a file\n",
    "with open('delete_augmented_images.sh', 'w') as script_file:\n",
    "    script_file.write(bash_script)\n",
    "\n",
    "# Make the script executable\n",
    "!chmod +x delete_augmented_images.sh\n",
    "\n",
    "# Execute the script\n",
    "!./delete_augmented_images.sh\n",
    "\n",
    "# Delete the script\n",
    "!rm delete_augmented_images.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata file\n",
    "metadata = pd.read_csv(FILEPATH_METADATA)\n",
    "\n",
    "# Concatenate the base directory with the image filename to add the full path\n",
    "metadata['image_path'] = FILEPATH_JPGS + metadata['image_id']\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data in train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train, validation and test using train_test_split\n",
    "\n",
    "# Split the data into two subsets: train and temp (60% train, 40% temp)\n",
    "train_df, temp_df = train_test_split(metadata, test_size=0.4, stratify=metadata[TARGET_LABEL], random_state=SEED)\n",
    "\n",
    "# Split the temp data into validation and test sets (50% each)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[TARGET_LABEL], random_state=SEED)\n",
    "\n",
    "# resetting the index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validation_df = validation_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "display(\n",
    "    train_df.shape,\n",
    "    validation_df.shape,\n",
    "    test_df.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling Class imbalances in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the image data generator for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using albumentations to augment the data\n",
    "\n",
    "datagen_augment = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    Rotate(limit=45, p=0.5),\n",
    "    RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    RandomResizedCrop(always_apply=False, p=0.5, scale=(0.75, 0.85), interpolation=0, height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]),\n",
    "    # HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    # GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "    # CoarseDropout(max_holes=12, min_height= 3 ,max_height=12,min_width=3, max_width=12, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames for every class  in the given BALANCE_LABEL column\n",
    "\n",
    "class_dataframes = {}\n",
    "for class_label in train_df[BALANCE_LABEL].unique():\n",
    "    class_dataframes[class_label] = train_df[train_df[BALANCE_LABEL] == class_label]\n",
    "    print(f\"Class {class_label} has {class_dataframes[class_label].shape[0]} samples\")\n",
    "    #display(class_dataframes[class_label].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to keep track of the number of augmented images per class\n",
    "class_augmentation_counts = {class_label: 0 for class_label in class_dataframes.keys()}\n",
    "\n",
    "# Create a list to store DataFrames for each class\n",
    "augmented_dataframes = []\n",
    "\n",
    "# Apply data augmentation for classes with few examples, trim classes with too many examples\n",
    "for class_label, class_df in class_dataframes.items():\n",
    "    \n",
    "    # Describing the overall progress\n",
    "    print(f\"\\nChecking class {class_label}...\")\n",
    "\n",
    "    # Calculate the number of images needed to reach MAX_SAMPLES for this class\n",
    "    images_needed = MAX_SAMPLES - class_df.shape[0]\n",
    "    \n",
    "    # If images_needed is negative, randomly select MAX_SAMPLES from the class_df\n",
    "    if images_needed < 0:\n",
    "        print(f\"> Result: Class {class_label} was reduced to {MAX_SAMPLES} samples\")\n",
    "        reduced_df = class_df.sample(n=MAX_SAMPLES, random_state=SEED)\n",
    "        augmented_dataframes.append(reduced_df)\n",
    "        continue\n",
    "    # If images_needed is zero, skip this class\n",
    "    elif images_needed == 0:\n",
    "        print(f\"> Result: Class {class_label} already has exactly {MAX_SAMPLES} samples\")\n",
    "        augmented_dataframes.append(class_df)\n",
    "        continue\n",
    "\n",
    "    # Generate augmented data - this part only runs if images_needed is positive\n",
    "    print(f\"> Result: Class {class_label} needs {images_needed} more images\")\n",
    "    augmented_dataframes.append(class_df)\n",
    "\n",
    "    while class_augmentation_counts[class_label] <= images_needed:\n",
    "\n",
    "        # Describing the subprocess progress for each class\n",
    "        sys.stdout.write(f\"\\rProgress: {class_augmentation_counts[class_label]}/{images_needed}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Randomly select an image from the class_df\n",
    "        i = random.randint(0, class_df.shape[0] - 1)\n",
    "        image_path = class_df.iloc[i]['image_path']\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        img = ih.img_load_and_transform(image_path, IMAGE_SIZE)\n",
    "\n",
    "        # Apply data augmentation via generator\n",
    "        augmented_img = datagen_augment(image=img)['image']\n",
    "\n",
    "        # Create a new image ID with prefix\n",
    "        augmented_image_id = f'aug_{ih.generate_random_string()}' + os.path.basename(image_path)\n",
    "\n",
    "        # Create a new image path with the augmented image ID as string\n",
    "        augmented_image_path = FILEPATH_JPGS + augmented_image_id\n",
    "        \n",
    "        # Create a new DataFrame for the augmented data for this instance only\n",
    "        augmented_instance_df = class_df.iloc[i:i+1].copy()\n",
    "        \n",
    "        # Reset the index of the new DataFrame\n",
    "        augmented_instance_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Update the \"image_id\" column with the augmented image ID\n",
    "        augmented_instance_df.at[0, 'image_id'] = augmented_image_id\n",
    "        \n",
    "        # Update the \"image_path\" column with the augmented image path\n",
    "        augmented_instance_df.at[0, 'image_path'] = augmented_image_path\n",
    "        \n",
    "        # Append the augmented DataFrame for this instance to the list\n",
    "        augmented_dataframes.append(augmented_instance_df)\n",
    "\n",
    "        # Save the augmented image to the output folder\n",
    "        augmented_image_path = os.path.join(FILEPATH_OUTPUT, augmented_image_id)\n",
    "        plt.imsave(augmented_image_path, augmented_img)\n",
    "\n",
    "        # Update the counter for the class\n",
    "        class_augmentation_counts[class_label] += 1\n",
    "\n",
    "# Combine all augmented DataFrames into a single DataFrame\n",
    "balanced_train_df = pd.concat(augmented_dataframes, ignore_index=True)\n",
    "\n",
    "balanced_train_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the image file folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what's going on in the folder\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    # Initialize counters\n",
    "    total_files = 0\n",
    "    aug_files = 0\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # List all files in the folder\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        # Count all files and files starting with \"aug_\"\n",
    "        for file in files:\n",
    "            total_files += 1\n",
    "            if file.startswith(\"aug_\"):\n",
    "                aug_files += 1\n",
    "\n",
    "        # Display the counts\n",
    "        print(f\"Total files in folder: {total_files}\")\n",
    "        print(f\"Files starting with 'aug_': {aug_files}\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' does not exist.\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"./../data/jpgs/\"  # Replace with your folder path\n",
    "count_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of 9 images from the balanced_train_df\n",
    "sample_df = balanced_train_df.sample(n=9, random_state=543)\n",
    "\n",
    "# Create a list of image paths from the \"image_path\" column\n",
    "image_paths = sample_df['image_path'].tolist()\n",
    "\n",
    "# Create a list of image labels from the \"dx\" column\n",
    "image_labels = sample_df['dx'].tolist()\n",
    "\n",
    "# Load and plot the images without imagehelper\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    img = plt.imread(image_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(image_labels[i])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Test, Validation and Training data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_df.to_csv('../data/processed/train_from_Metadata_processed.csv', index=False)\n",
    "validation_df.to_csv('../data/processed/validation_from_Metadata_processed.csv', index=False)\n",
    "test_df.to_csv('../data/processed/test_from_Metadata_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
